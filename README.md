# Flink Fintech POC - Real-time Analytics Pipeline

M·ªôt h·ªá th·ªëng real-time analytics ho√†n ch·ªânh cho fintech, s·ª≠ d·ª•ng PostgreSQL, Kafka, Debezium, Apache Flink v√† Grafana ƒë·ªÉ x·ª≠ l√Ω v√† ph√¢n t√≠ch d·ªØ li·ªáu t√†i ch√≠nh theo th·ªùi gian th·ª±c.

## üìä T√≥m T·∫Øt Lu·ªìng D·ªØ Li·ªáu

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   app-python    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   PostgreSQL    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Debezium     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ      Kafka      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Flink Jobs    ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ   (CDC)         ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ Data Generator  ‚îÇ    ‚îÇ Core Tables     ‚îÇ    ‚îÇ Outbox Pattern  ‚îÇ    ‚îÇ Topics          ‚îÇ    ‚îÇ Real-time       ‚îÇ
‚îÇ + Producer      ‚îÇ    ‚îÇ + Outbox Table  ‚îÇ    ‚îÇ + Direct CDC    ‚îÇ    ‚îÇ (10+ topics)    ‚îÇ    ‚îÇ Processing      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ                                                              ‚îÇ
                                ‚îÇ                                                              ‚ñº
                                ‚îÇ                                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                ‚îÇ                                                    ‚îÇ     Grafana     ‚îÇ
                                ‚îÇ                                                    ‚îÇ   Dashboards    ‚îÇ
                                ‚îÇ                                                    ‚îÇ   Real-time     ‚îÇ
                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Visualization  ‚îÇ
                                                                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Lu·ªìng d·ªØ li·ªáu ch√≠nh:**
1. **Data Generation**: Python app t·∫°o d·ªØ li·ªáu fintech realistic
2. **Database Storage**: L∆∞u v√†o PostgreSQL v·ªõi outbox pattern
3. **CDC Streaming**: Debezium capture changes v√† stream ra Kafka
4. **Real-time Processing**: Flink x·ª≠ l√Ω d·ªØ li·ªáu theo th·ªùi gian th·ª±c
5. **Analytics Output**: K·∫øt qu·∫£ ƒë∆∞·ª£c g·ª≠i ƒë·∫øn Grafana dashboard

## üèóÔ∏è Ki·∫øn Tr√∫c App-Python, Database v√† Data Generation

### **App-Python Architecture**

#### **Core Components:**
- **`main.py`**: Entry point v√† CLI commands
- **`realtime_producer.py`**: Producer ch√≠nh t·∫°o d·ªØ li·ªáu li√™n t·ª•c
- **`data_generator.py`**: Generator d·ªØ li·ªáu realistic v·ªõi behavioral patterns
- **`models.py`**: Pydantic models ƒë·ªãnh nghƒ©a data structures
- **`connectors.py`**: Database connections v√† Debezium connector setup
- **`config.py`**: Configuration settings

#### **Data Generation Strategy:**
```python
# Realistic behavioral patterns
transaction_patterns = {
    CustomerTier.BASIC: {'daily_txns': (1, 5), 'avg_amount': (10, 500)},
    CustomerTier.PREMIUM: {'daily_txns': (3, 15), 'avg_amount': (50, 2000)},
    CustomerTier.VIP: {'daily_txns': (5, 25), 'avg_amount': (100, 10000)},
    CustomerTier.ENTERPRISE: {'daily_txns': (10, 100), 'avg_amount': (1000, 100000)}
}
```

### **Database Design**

#### **Core Business Tables:**
| B·∫£ng | M·ª•c ƒë√≠ch | D·ªØ li·ªáu ch√≠nh |
|------|----------|----------------|
| `customers` | Qu·∫£n l√Ω kh√°ch h√†ng | Th√¥ng tin c√° nh√¢n, tier, risk_score, credit_score |
| `accounts` | Qu·∫£n l√Ω t√†i kho·∫£n | Lo·∫°i t√†i kho·∫£n, s·ªë d∆∞, credit_limit, interest_rate |
| `transactions` | Giao d·ªãch t√†i ch√≠nh | Lo·∫°i giao d·ªãch, s·ªë ti·ªÅn, merchant, risk_score, compliance_flags |
| `merchants` | Th√¥ng tin th∆∞∆°ng nh√¢n | T√™n c√¥ng ty, lo·∫°i h√¨nh, MCC codes, ƒë·ªãa ch·ªâ |
| `fraud_alerts` | C·∫£nh b√°o gian l·∫≠n | Lo·∫°i c·∫£nh b√°o, severity, confidence_score, rules_triggered |

#### **Analytics Tables:**
| B·∫£ng | M·ª•c ƒë√≠ch | D·ªØ li·ªáu ch√≠nh |
|------|----------|----------------|
| `account_balances` | Bi·∫øn ƒë·ªông s·ªë d∆∞ | S·ªë d∆∞ theo th·ªùi gian, pending_debits |
| `customer_sessions` | Phi√™n l√†m vi·ªác | Channel, device, IP, location, actions_count |
| `market_data` | D·ªØ li·ªáu th·ªã tr∆∞·ªùng | Symbol, price, change, volume, market_cap |

#### **Outbox Pattern Table:**
| B·∫£ng | M·ª•c ƒë√≠ch | D·ªØ li·ªáu ch√≠nh |
|------|----------|----------------|
| `outbox` | Business events | aggregate_type, aggregate_id, event_type, payload (JSON) |

### **Data Generation Process**

#### **1. Initial Data Generation:**
```python
# Generate realistic customer base
scenario_data = self.data_generator.generate_realistic_scenario(self.num_customers)
# Creates interconnected customers, accounts, merchants
```

#### **2. Real-time Data Production:**
```python
# Continuous transaction generation
async def _transaction_generator_loop(self):
    while self.is_running:
        await self._generate_and_send_transaction()
        await asyncio.sleep(interval)
```

#### **3. Outbox Pattern Implementation:**
```python
# Store transaction + outbox events atomically
await self._store_transaction(transaction)
await self._store_outbox_events(transaction, customer, account)
```

#### **4. Logic T·∫°o Data Fake (Th·ª±c T·∫ø t·ª´ Code):**

##### **A) Customer Generation v·ªõi Behavioral Patterns:**
```python
# Customer tier distribution v·ªõi behavioral characteristics
customer_tiers = {
    'BASIC': 60%,      # 60% kh√°ch h√†ng basic
    'PREMIUM': 25%,    # 25% kh√°ch h√†ng premium  
    'VIP': 10%,        # 10% kh√°ch h√†ng VIP
    'ENTERPRISE': 5%   # 5% kh√°ch h√†ng enterprise
}

# Risk score calculation d·ª±a tr√™n tier
base_risk = {
    CustomerTier.BASIC: 20,
    CustomerTier.PREMIUM: 15,
    CustomerTier.VIP: 10,
    CustomerTier.ENTERPRISE: 5
}
risk_score = base_risk[tier] + random.gauss(0, 10)
```

##### **B) Transaction Generation v·ªõi Realistic Patterns:**
```python
# Time-based transaction rates (theo gi·ªù trong ng√†y)
def _get_time_based_transaction_rate(self) -> float:
    current_hour = datetime.now().hour
    
    if 9 <= current_hour <= 17:      # Business hours: 1.5x - 2.0x
        return self.production_rate * random.uniform(1.5, 2.0)
    elif 18 <= current_hour <= 22:   # Evening peak: 1.2x - 1.8x
        return self.production_rate * random.uniform(1.2, 1.8)
    elif 6 <= current_hour <= 8:     # Morning: 0.8x - 1.2x
        return self.production_rate * random.uniform(0.8, 1.2)
    else:                            # Night/early morning: 0.2x - 0.5x
        return self.production_rate * random.uniform(0.2, 0.5)

# Amount distribution theo customer tier
amount_patterns = {
    'BASIC': {'min': 10, 'max': 500, 'avg': 100},
    'PREMIUM': {'min': 50, 'max': 2000, 'avg': 500},
    'VIP': {'min': 100, 'max': 10000, 'avg': 2000},
    'ENTERPRISE': {'min': 1000, 'max': 100000, 'avg': 25000}
}
```

##### **C) Fraud Detection Logic (Th·ª±c T·∫ø t·ª´ Code):**
```python
# ƒêi·ªÅu ki·ªán t·∫°o fraud alert (t·ª´ realtime_producer.py)
if transaction.risk_level in [RiskLevel.HIGH, RiskLevel.CRITICAL] and random.random() > 0.6:
    fraud_alert = self.data_generator.generate_fraud_alert(customer, transaction)

# Fraud alert generation (t·ª´ data_generator.py)
alert_types = [
    'Unusual spending pattern', 'Geographic anomaly', 'Velocity check failed',
    'Device mismatch', 'Time-based anomaly', 'Amount threshold exceeded',
    'Merchant blacklist match', 'Card testing detected'
]

# Random generation v·ªõi weighted distribution
alert = FraudAlert(
    alert_type=random.choice(alert_types),  # Random ch·ªçn 1 type
    severity=random.choices([LOW, MEDIUM, HIGH, CRITICAL], weights=[10, 30, 40, 20])[0],
    confidence_score=random.uniform(0.5, 1.0),  # Random 0.5-1.0
    rules_triggered=[f"RULE_{random.randint(100, 999)}" for _ in range(random.randint(1, 3))]
)
```

##### **D) Compliance Flags Generation (Th·ª±c T·∫ø t·ª´ Code):**
```python
def _generate_compliance_flags(self, risk_score: float, amount: Decimal) -> List[str]:
    flags = []
    
    if amount > Decimal('10000'):
        flags.append('HIGH_VALUE')
    
    if risk_score > 70:
        flags.append('HIGH_RISK')
    
    if amount > Decimal('3000') and random.random() > 0.8:
        flags.append('STRUCTURING_SUSPECTED')
    
    if random.random() > 0.95:
        flags.append('SANCTIONS_CHECK_REQUIRED')
    
    return flags
```

##### **E) Risk Scoring Logic (Th·ª±c T·∫ø t·ª´ Code):**
```python
# Risk scoring trong generate_transaction
risk_factors = 0
if amount > Decimal('10000'):
    risk_factors += 20
if customer.risk_score > 50:
    risk_factors += 10
if transaction_type == TransactionType.WIRE_TRANSFER:
    risk_factors += 15

risk_score = min(100, risk_factors + random.uniform(0, 20))
risk_level = RiskLevel.CRITICAL if risk_score > 80 else \
            RiskLevel.HIGH if risk_score > 60 else \
            RiskLevel.MEDIUM if risk_score > 30 else RiskLevel.LOW
```

##### **F) Customer Selection Logic (Th·ª±c T·∫ø t·ª´ Code):**
```python
def _select_customer_for_transaction(self) -> Customer:
    # Higher tier customers are more active
    weights = []
    for customer in self.customers:
        if customer.tier == CustomerTier.ENTERPRISE:
            weight = 4.0
        elif customer.tier == CustomerTier.VIP:
            weight = 3.0
        elif customer.tier == CustomerTier.PREMIUM:
            weight = 2.0
        else:
            weight = 1.0
        weights.append(weight)
    
    return random.choices(self.customers, weights=weights)[0]
```

## üîå Kafka Topics v√† Debezium Connectors

### **3 Debezium Connectors**

#### **1. Main Connector (`fintech-main-connector`):**
- **B·∫£ng ƒë∆∞·ª£c monitor**: `customers`, `accounts`, `transactions`, `merchants`, `fraud_alerts`
- **Topics**: `fintech.customers`, `fintech.accounts`, `fintech.transactions`, `fintech.merchants`, `fintech.fraud_alerts`
- **M·ª•c ƒë√≠ch**: Stream tr·ª±c ti·∫øp thay ƒë·ªïi d·ªØ li·ªáu t·ª´ c√°c b·∫£ng ch√≠nh

#### **2. Outbox Connector (`fintech-outbox-connector`):**
- **B·∫£ng ƒë∆∞·ª£c monitor**: `outbox`
- **Topics**: `fintech.events.*` (ƒë∆∞·ª£c route t·ª± ƒë·ªông theo event_type)
- **Transformation**: `io.debezium.transforms.outbox.EventRouter`
- **M·ª•c ƒë√≠ch**: Stream business events v·ªõi context ƒë·∫ßy ƒë·ªß

#### **3. Analytics Connector (`fintech-analytics-connector`):**
- **B·∫£ng ƒë∆∞·ª£c monitor**: `account_balances`, `customer_sessions`
- **Topics**: `fintech.analytics.*`
- **M·ª•c ƒë√≠ch**: Stream d·ªØ li·ªáu analytics v√† metrics

### **Kafka Topics Structure**

```
fintech.customers          # Customer data changes
fintech.accounts           # Account data changes  
fintech.transactions       # Transaction data changes
fintech.merchants          # Merchant data changes
fintech.fraud_alerts       # Fraud alert data changes
fintech.events.*           # Business events (routed by event_type)
fintech.analytics.*        # Analytics data
```

## ‚ö° Flink Jobs - Real-time Processing

### **Main Analytics Job: `FintechAnalyticsJob`**

#### **Input Streams:**
- **Transaction Stream**: Giao d·ªãch real-time t·ª´ `fintech.transactions`
- **Customer Stream**: Th√¥ng tin kh√°ch h√†ng t·ª´ `fintech.customers`
- **Fraud Alert Stream**: C·∫£nh b√°o gian l·∫≠n t·ª´ `fintech.fraud_alerts`
- **Events Stream**: Business events t·ª´ `fintech.events`

#### **Processing Streams (10 Analytics Pipelines):**

| Stream | Input | Logic | Time Window | Output |
|--------|-------|-------|-------------|---------|
| **Fraud Detection** | Transactions | Ph√¢n t√≠ch giao d·ªãch ƒë·ªÉ ph√°t hi·ªán gian l·∫≠n | Real-time | `FraudDetectionResult` |
| **Transaction Metrics** | Transactions | Metrics theo customer | 5 ph√∫t | `TransactionMetrics` |
| **Customer Behavior** | Transactions | Ph√¢n t√≠ch h√†nh vi kh√°ch h√†ng | 15 ph√∫t | `CustomerBehaviorMetrics` |
| **Risk Analytics** | Transactions | ƒê√°nh gi√° r·ªßi ro theo customer | 10 ph√∫t | `RiskAnalytics` |
| **Dashboard Metrics** | Transactions | Metrics t·ªïng h·ª£p cho dashboard | 1 ph√∫t | `DashboardMetrics` |
| **Anomaly Detection** | Transactions | Ph√°t hi·ªán b·∫•t th∆∞·ªùng | Real-time | `AnomalyAlert` |
| **Geographic Analysis** | Transactions | Ph√¢n t√≠ch theo ƒë·ªãa l√Ω | 5 ph√∫t | `GeographicMetrics` |
| **Merchant Analysis** | Transactions | Ph√¢n t√≠ch theo merchant | 10 ph√∫t | `MerchantMetrics` |
| **Time Pattern Analysis** | Transactions | Pattern theo gi·ªù | 1 gi·ªù | `TimePatternMetrics` |
| **Compliance Monitoring** | Transactions | Gi√°m s√°t compliance | 30 ph√∫t | `ComplianceMetrics` |

#### **Logic Chi Ti·∫øt c·ªßa T·ª´ng Flink Job:**

##### **1. Fraud Detection Job:**
```java
// Logic: Ph√¢n t√≠ch giao d·ªãch real-time ƒë·ªÉ ph√°t hi·ªán gian l·∫≠n
public class FraudDetectionFunction extends RichMapFunction<Transaction, FraudDetectionResult> {
    
    // State: L∆∞u tr·ªØ pattern c·ªßa t·ª´ng customer
    private ValueState<CustomerPatternState> customerPatternState;
    
    public FraudDetectionResult map(Transaction transaction) {
        CustomerPatternState currentState = customerPatternState.value();
        
        // Fraud detection rules:
        // 1. Amount threshold check
        if (transaction.getAmount().compareTo(currentState.getMaxNormalAmount()) > 0) {
            return new FraudDetectionResult(transaction.getId(), "HIGH_AMOUNT", 0.8);
        }
        
        // 2. Frequency anomaly check
        if (currentState.getTransactionCount() > currentState.getMaxTransactionsPerHour()) {
            return new FraudDetectionResult(transaction.getId(), "FREQUENCY_ANOMALY", 0.7);
        }
        
        // 3. Location anomaly check
        if (!currentState.getNormalLocations().contains(transaction.getTransactionLocation())) {
            return new FraudDetectionResult(transaction.getId(), "LOCATION_ANOMALY", 0.6);
        }
        
        // 4. Time pattern anomaly
        if (!currentState.getNormalTransactionHours().contains(transaction.getCreatedAt().getHours())) {
            return new FraudDetectionResult(transaction.getId(), "TIME_ANOMALY", 0.5);
        }
        
        return new FraudDetectionResult(transaction.getId(), "NORMAL", 0.1);
    }
}
```

##### **2. Transaction Metrics Job:**
```java
// Logic: T√≠nh to√°n metrics theo customer trong window 5 ph√∫t
public class TransactionMetricsProcessor extends ProcessWindowFunction<Transaction, TransactionMetrics, String, TimeWindow> {
    
    public void process(String customerId, Context context, Iterable<Transaction> transactions, Collector<TransactionMetrics> out) {
        
        // T√≠nh to√°n metrics:
        BigDecimal totalAmount = BigDecimal.ZERO;
        BigDecimal totalFees = BigDecimal.ZERO;
        int transactionCount = 0;
        Map<String, Integer> typeDistribution = new HashMap<>();
        
        for (Transaction t : transactions) {
            totalAmount = totalAmount.add(t.getAmount());
            totalFees = totalFees.add(t.getFeeAmount());
            transactionCount++;
            
            // Type distribution
            String type = t.getTransactionType().toString();
            typeDistribution.put(type, typeDistribution.getOrDefault(type, 0) + 1);
        }
        
        // T√≠nh average v√† trends
        BigDecimal avgAmount = totalAmount.divide(BigDecimal.valueOf(transactionCount), 2, RoundingMode.HALF_UP);
        
        TransactionMetrics metrics = new TransactionMetrics(
            customerId, context.window().getStart(), context.window().getEnd(),
            transactionCount, totalAmount, avgAmount, totalFees, typeDistribution
        );
        
        out.collect(metrics);
    }
}
```

##### **3. Customer Behavior Job:**
```java
// Logic: Ph√¢n t√≠ch h√†nh vi kh√°ch h√†ng trong window 15 ph√∫t
public class CustomerBehaviorProcessor extends ProcessWindowFunction<Transaction, CustomerBehaviorMetrics, String, TimeWindow> {
    
    public void process(String customerId, Context context, Iterable<Transaction> transactions, Collector<CustomerBehaviorMetrics> out) {
        
        // Behavioral analysis:
        Map<String, Integer> channelUsage = new HashMap<>();
        Map<String, BigDecimal> spendingByCategory = new HashMap<>();
        List<String> locations = new ArrayList<>();
        List<String> devices = new ArrayList<>();
        
        for (Transaction t : transactions) {
            // Channel analysis
            String channel = t.getNetwork(); // mobile, web, atm
            channelUsage.put(channel, channelUsage.getOrDefault(channel, 0) + 1);
            
            // Category spending
            String category = getMerchantCategory(t.getMerchantId());
            spendingByCategory.merge(category, t.getAmount(), BigDecimal::add);
            
            // Location tracking
            if (t.getTransactionLocation() != null) {
                locations.add(t.getTransactionLocation().get("city"));
            }
            
            // Device tracking
            if (t.getDeviceFingerprint() != null) {
                devices.add(t.getDeviceFingerprint());
            }
        }
        
        // Calculate behavioral scores
        double locationConsistency = calculateLocationConsistency(locations);
        double deviceConsistency = calculateDeviceConsistency(devices);
        double spendingPattern = calculateSpendingPattern(spendingByCategory);
        
        CustomerBehaviorMetrics behavior = new CustomerBehaviorMetrics(
            customerId, context.window().getStart(), context.window().getEnd(),
            channelUsage, spendingByCategory, locationConsistency, 
            deviceConsistency, spendingPattern
        );
        
        out.collect(behavior);
    }
}
```

##### **4. Risk Analytics Job:**
```java
// Logic: ƒê√°nh gi√° r·ªßi ro theo customer trong window 10 ph√∫t
public class RiskAnalyticsProcessor extends ProcessWindowFunction<Transaction, RiskAnalytics, String, TimeWindow> {
    
    public void process(String customerId, Context context, Iterable<Transaction> transactions, Collector<RiskAnalytics> out) {
        
        // Risk assessment factors:
        BigDecimal totalExposure = BigDecimal.ZERO;
        int highRiskTransactions = 0;
        List<String> complianceViolations = new ArrayList<>();
        Map<String, Integer> riskFactors = new HashMap<>();
        
        for (Transaction t : transactions) {
            // Exposure calculation
            totalExposure = totalExposure.add(t.getAmount());
            
            // High risk transactions
            if (t.getRiskLevel() == RiskLevel.HIGH || t.getRiskLevel() == RiskLevel.CRITICAL) {
                highRiskTransactions++;
            }
            
            // Compliance violations
            if (!t.getComplianceFlags().isEmpty()) {
                complianceViolations.addAll(t.getComplianceFlags());
            }
            
            // Risk factor analysis
            analyzeRiskFactors(t, riskFactors);
        }
        
        // Calculate risk score
        double riskScore = calculateRiskScore(
            totalExposure, highRiskTransactions, 
            complianceViolations.size(), riskFactors
        );
        
        RiskAnalytics risk = new RiskAnalytics(
            customerId, context.window().getStart(), context.window().getEnd(),
            riskScore, totalExposure, highRiskTransactions, 
            complianceViolations, riskFactors
        );
        
        out.collect(risk);
    }
}
```

##### **5. Dashboard Metrics Job:**
```java
// Logic: Metrics t·ªïng h·ª£p cho dashboard trong window 1 ph√∫t
public class DashboardMetricsProcessor extends ProcessWindowFunction<Transaction, DashboardMetrics, String, TimeWindow> {
    
    public void process(String transactionType, Context context, Iterable<Transaction> transactions, Collector<DashboardMetrics> out) {
        
        // Real-time dashboard metrics:
        BigDecimal totalVolume = BigDecimal.ZERO;
        int transactionCount = 0;
        BigDecimal totalFees = BigDecimal.ZERO;
        Map<String, Integer> statusDistribution = new HashMap<>();
        Map<String, Integer> riskDistribution = new HashMap<>();
        
        for (Transaction t : transactions) {
            totalVolume = totalVolume.add(t.getAmount());
            transactionCount++;
            totalFees = totalFees.add(t.getFeeAmount());
            
            // Status distribution
            String status = t.getStatus().toString();
            statusDistribution.put(status, statusDistribution.getOrDefault(status, 0) + 1);
            
            // Risk distribution
            String risk = t.getRiskLevel().toString();
            riskDistribution.put(risk, riskDistribution.getOrDefault(risk, 0) + 1);
        }
        
        // Calculate KPIs
        BigDecimal avgTransactionValue = totalVolume.divide(BigDecimal.valueOf(transactionCount), 2, RoundingMode.HALF_UP);
        double successRate = (double) statusDistribution.getOrDefault("COMPLETED", 0) / transactionCount;
        
        DashboardMetrics metrics = new DashboardMetrics(
            transactionType, context.window().getStart(), context.window().getEnd(),
            transactionCount, totalVolume, avgTransactionValue, totalFees,
            successRate, statusDistribution, riskDistribution
        );
        
        out.collect(metrics);
    }
}
```

##### **6. Anomaly Detection Job:**
```java
// Logic: Ph√°t hi·ªán b·∫•t th∆∞·ªùng real-time
public class AnomalyDetectionFunction extends RichMapFunction<Transaction, AnomalyAlert> {
    
    private ValueState<CustomerPatternState> customerPatternState;
    
    public AnomalyAlert map(Transaction transaction) {
        CustomerPatternState currentState = customerPatternState.value();
        
        // Anomaly detection algorithms:
        
        // 1. Statistical outlier detection (Z-score)
        double zScore = calculateZScore(transaction.getAmount(), currentState.getAmountMean(), currentState.getAmountStdDev());
        if (Math.abs(zScore) > 3.0) {
            return new AnomalyAlert(transaction.getId(), "STATISTICAL_OUTLIER", zScore, "Amount deviation");
        }
        
        // 2. Pattern deviation detection
        if (isPatternDeviation(transaction, currentState)) {
            return new AnomalyAlert(transaction.getId(), "PATTERN_DEVIATION", 0.8, "Behavioral change");
        }
        
        // 3. Velocity anomaly detection
        if (isVelocityAnomaly(transaction, currentState)) {
            return new AnomalyAlert(transaction.getId(), "VELOCITY_ANOMALY", 0.7, "Transaction frequency change");
        }
        
        return null; // No anomaly detected
    }
}
```

##### **7. Geographic Analysis Job:**
```java
// Logic: Ph√¢n t√≠ch theo ƒë·ªãa l√Ω trong window 5 ph√∫t
public class GeographicMetricsProcessor extends ProcessWindowFunction<Transaction, GeographicMetrics, String, TimeWindow> {
    
    public void process(String country, Context context, Iterable<Transaction> transactions, Collector<GeographicMetrics> out) {
        
        // Geographic analysis:
        Map<String, Integer> cityDistribution = new HashMap<>();
        Map<String, BigDecimal> spendingByCity = new HashMap<>();
        Map<String, Integer> merchantDistribution = new HashMap<>();
        List<String> currencies = new ArrayList<>();
        
        for (Transaction t : transactions) {
            if (t.getTransactionLocation() != null) {
                String city = t.getTransactionLocation().get("city");
                cityDistribution.put(city, cityDistribution.getOrDefault(city, 0) + 1);
                
                // Spending by city
                spendingByCity.merge(city, t.getAmount(), BigDecimal::add);
                
                // Merchant distribution
                if (t.getMerchantId() != null) {
                    String merchantType = getMerchantType(t.getMerchantId());
                    merchantDistribution.put(merchantType, merchantDistribution.getOrDefault(merchantType, 0) + 1);
                }
                
                // Currency tracking
                currencies.add(t.getCurrency());
            }
        }
        
        GeographicMetrics geo = new GeographicMetrics(
            country, context.window().getStart(), context.window().getEnd(),
            cityDistribution, spendingByCity, merchantDistribution, currencies
        );
        
        out.collect(geo);
    }
}
```

##### **8. Merchant Analysis Job:**
```java
// Logic: Ph√¢n t√≠ch theo merchant trong window 10 ph√∫t
public class MerchantMetricsProcessor extends ProcessWindowFunction<Transaction, MerchantMetrics, String, TimeWindow> {
    
    public void process(String merchantId, Context context, Iterable<Transaction> transactions, Collector<MerchantMetrics> out) {
        
        // Merchant performance analysis:
        BigDecimal totalRevenue = BigDecimal.ZERO;
        int transactionCount = 0;
        Map<String, Integer> customerDistribution = new HashMap<>();
        Map<String, BigDecimal> amountDistribution = new HashMap<>();
        List<String> locations = new ArrayList<>();
        
        for (Transaction t : transactions) {
            totalRevenue = totalRevenue.add(t.getAmount());
            transactionCount++;
            
            // Customer distribution
            customerDistribution.put(t.getCustomerId(), customerDistribution.getOrDefault(t.getCustomerId(), 0) + 1);
            
            // Amount distribution by range
            String amountRange = getAmountRange(t.getAmount());
            amountDistribution.merge(amountRange, t.getAmount(), BigDecimal::add);
            
            // Location tracking
            if (t.getTransactionLocation() != null) {
                locations.add(t.getTransactionLocation().get("city"));
            }
        }
        
        // Calculate merchant KPIs
        BigDecimal avgTransactionValue = totalRevenue.divide(BigDecimal.valueOf(transactionCount), 2, RoundingMode.HALF_UP);
        int uniqueCustomers = customerDistribution.size();
        double customerRetention = calculateCustomerRetention(customerDistribution);
        
        MerchantMetrics merchant = new MerchantMetrics(
            merchantId, context.window().getStart(), context.window().getEnd(),
            totalRevenue, transactionCount, avgTransactionValue, uniqueCustomers,
            customerRetention, customerDistribution, amountDistribution, locations
        );
        
        out.collect(merchant);
    }
}
```

##### **9. Time Pattern Analysis Job:**
```java
// Logic: Ph√¢n t√≠ch pattern theo gi·ªù trong window 1 gi·ªù
public class TimePatternProcessor extends ProcessWindowFunction<Transaction, TimePatternMetrics, Integer, TimeWindow> {
    
    public void process(Integer hour, Context context, Iterable<Transaction> transactions, Collector<TimePatternMetrics> out) {
        
        // Time-based pattern analysis:
        Map<String, Integer> transactionTypeDistribution = new HashMap<>();
        Map<String, BigDecimal> spendingByType = new HashMap<>();
        Map<String, Integer> customerActivity = new HashMap<>();
        List<String> peakPeriods = new ArrayList<>();
        
        for (Transaction t : transactions) {
            // Transaction type distribution by hour
            String type = t.getTransactionType().toString();
            transactionTypeDistribution.put(type, transactionTypeDistribution.getOrDefault(type, 0) + 1);
            
            // Spending patterns by type
            spendingByType.merge(type, t.getAmount(), BigDecimal::add);
            
            // Customer activity patterns
            customerActivity.put(t.getCustomerId(), customerActivity.getOrDefault(t.getCustomerId(), 0) + 1);
        }
        
        // Identify peak periods
        if (hour >= 9 && hour <= 11) peakPeriods.add("MORNING_PEAK");
        if (hour >= 12 && hour <= 14) peakPeriods.add("LUNCH_PEAK");
        if (hour >= 17 && hour <= 19) peakPeriods.add("EVENING_PEAK");
        
        TimePatternMetrics timePattern = new TimePatternMetrics(
            hour, context.window().getStart(), context.window().getEnd(),
            transactionTypeDistribution, spendingByType, customerActivity, peakPeriods
        );
        
        out.collect(timePattern);
    }
}
```

##### **10. Compliance Monitoring Job:**
```java
// Logic: Gi√°m s√°t compliance theo customer trong window 30 ph√∫t
public class ComplianceProcessor extends ProcessWindowFunction<Transaction, ComplianceMetrics, String, TimeWindow> {
    
    public void process(String customerId, Context context, Iterable<Transaction> transactions, Collector<ComplianceMetrics> out) {
        
        // Compliance monitoring:
        Map<String, Integer> violationTypes = new HashMap<>();
        List<String> flaggedTransactions = new ArrayList<>();
        BigDecimal totalViolationAmount = BigDecimal.ZERO;
        Map<String, Integer> riskLevelDistribution = new HashMap<>();
        
        for (Transaction t : transactions) {
            // Compliance violations
            for (String flag : t.getComplianceFlags()) {
                violationTypes.put(flag, violationTypes.getOrDefault(flag, 0) + 1);
                flaggedTransactions.add(t.getId());
                totalViolationAmount = totalViolationAmount.add(t.getAmount());
            }
            
            // Risk level distribution
            String riskLevel = t.getRiskLevel().toString();
            riskLevelDistribution.put(riskLevel, riskLevelDistribution.getOrDefault(riskLevel, 0) + 1);
        }
        
        // Calculate compliance score
        double complianceScore = calculateComplianceScore(violationTypes, flaggedTransactions.size());
        boolean requiresReview = complianceScore < 0.7;
        
        ComplianceMetrics compliance = new ComplianceMetrics(
            customerId, context.window().getStart(), context.window().getEnd(),
            complianceScore, requiresReview, violationTypes, flaggedTransactions,
            totalViolationAmount, riskLevelDistribution
        );
        
        out.collect(compliance);
    }
}
```

### **Job Configuration**

#### **Flink Settings:**
```yaml
# flink-conf.yaml
jobmanager.rpc.address: localhost
taskmanager.numberOfTaskSlots: 4
parallelism.default: 2
state.backend: rocksdb
checkpointing.interval: 10000ms
```

#### **Build v√† Run:**
```bash
# Build
cd flink-jobs
mvn clean package

# Run
flink run target/flink-jobs-1.0-SNAPSHOT.jar
```

## üìà Grafana Integration v√† Data Visualization

### **Data Flow to Grafana**

#### **1. Flink Output Streams:**
```java
// C√°c streams ƒë∆∞·ª£c x·ª≠ l√Ω real-time
DataStream<DashboardMetrics> dashboardMetricsStream = 
    transactionStream.keyBy(Transaction::getTransactionType)
                    .window(TumblingEventTimeWindows.of(Time.minutes(1)))
                    .process(new DashboardMetricsProcessor());
```

#### **2. Kafka Sink (Output Topics):**
- **`fintech.analytics.fraud_detection`**: K·∫øt qu·∫£ ph√°t hi·ªán gian l·∫≠n
- **`fintech.analytics.transaction_metrics`**: Metrics giao d·ªãch
- **`fintech.analytics.customer_behavior`**: Ph√¢n t√≠ch h√†nh vi kh√°ch h√†ng
- **`fintech.analytics.risk_analytics`**: Ph√¢n t√≠ch r·ªßi ro
- **`fintech.analytics.dashboard_metrics`**: Metrics cho dashboard

#### **3. Grafana Data Sources:**
- **Kafka**: Real-time streaming data
- **PostgreSQL**: Historical data v√† reference data
- **Prometheus**: Metrics v√† monitoring data

### **Dashboard Components**

#### **Real-time Dashboards:**
1. **Transaction Monitor**: S·ªë l∆∞·ª£ng giao d·ªãch theo th·ªùi gian th·ª±c
2. **Fraud Detection**: C·∫£nh b√°o gian l·∫≠n v√† risk scores
3. **Customer Analytics**: H√†nh vi v√† metrics kh√°ch h√†ng
4. **Risk Management**: Risk scores v√† compliance monitoring
5. **Geographic Analysis**: Ph√¢n t√≠ch theo ƒë·ªãa l√Ω
6. **Merchant Performance**: Hi·ªáu su·∫•t th∆∞∆°ng nh√¢n

#### **Key Metrics Displayed:**
- **Transaction Volume**: S·ªë l∆∞·ª£ng giao d·ªãch theo th·ªùi gian
- **Risk Scores**: Risk levels v√† trends
- **Fraud Alerts**: S·ªë l∆∞·ª£ng v√† severity c·ªßa c·∫£nh b√°o
- **Customer Behavior**: Spending patterns v√† engagement
- **Geographic Distribution**: Ph√¢n b·ªë giao d·ªãch theo v√πng
- **Compliance Status**: Compliance flags v√† violations

### **Real-time Updates**

#### **Update Frequency:**
- **Dashboard Metrics**: C·∫≠p nh·∫≠t m·ªói ph√∫t
- **Transaction Metrics**: C·∫≠p nh·∫≠t m·ªói 5 ph√∫t
- **Customer Behavior**: C·∫≠p nh·∫≠t m·ªói 15 ph√∫t
- **Risk Analytics**: C·∫≠p nh·∫≠t m·ªói 10 ph√∫t

#### **Data Freshness:**
- **End-to-end latency**: < 1 gi√¢y
- **Data consistency**: ƒê·∫£m b·∫£o b·ªüi Flink checkpointing
- **Real-time processing**: Kh√¥ng c√≥ batch delays

## üöÄ Quick Start

### **1. Start Infrastructure:**
```bash
cd docker
docker-compose up -d
```

### **2. Setup Python App:**
```bash
cd app-python
python main.py run-all --rate 10 --num-customers 100
```

### **3. Run Flink Job:**
```bash
cd flink-jobs
mvn clean package
flink run target/flink-jobs-1.0-SNAPSHOT.jar
```

### **4. Access Dashboards:**
- **Flink UI**: http://localhost:8081
- **Kafka UI**: http://localhost:8080
- **Grafana**: http://localhost:3000 (admin/admin)

## üîß Architecture Benefits

### **1. Real-time Processing:**
- **Sub-second latency** cho fraud detection
- **Continuous processing** kh√¥ng c√≥ batch delays
- **Real-time analytics** cho business decisions

### **2. Scalability:**
- **Horizontal scaling** v·ªõi Flink parallelism
- **Kafka partitioning** cho throughput cao
- **Database sharding** support

### **3. Reliability:**
- **Outbox pattern** ƒë·∫£m b·∫£o event delivery
- **Flink checkpointing** cho fault tolerance
- **Debezium CDC** cho data consistency

### **4. Flexibility:**
- **Multiple data sources** (tables + outbox)
- **Configurable processing** v·ªõi time windows
- **Extensible analytics** pipelines

## üìö Technologies Used

- **Data Generation**: Python, Faker, Pydantic
- **Database**: PostgreSQL v·ªõi logical replication
- **CDC**: Debezium v·ªõi outbox pattern
- **Streaming**: Apache Kafka
- **Processing**: Apache Flink 1.18.1
- **Visualization**: Grafana
- **Infrastructure**: Docker, Docker Compose

---

**T√°c gi·∫£**: Flink Fintech POC Team  
**Phi√™n b·∫£n**: 1.0  
**C·∫≠p nh·∫≠t**: 2024
